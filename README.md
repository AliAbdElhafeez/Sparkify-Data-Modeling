# Sparkify Data Modeling with Postgres and Apache Cassandra


## Introduction

This is the first project in Udacity's Data Engineering Nanodegree. This data modeling project consists of two parts:

a) We'd like to create a Postgres database with tables designed to optimize queries on song play analysis. My role in this part is to create a database schema and ETL pipeline for this analysis.

b) We'd like to create an Apache Cassandra database which can create queries on song play data to answer questions. My role in this part is to create a database for this analysis based on the queries obtained from the questions mentioned below.

# First part 

### Data Modeling with Postgres

In this first part of the project, We are going to create a Postgres database with tables. To do this we need to define fact and dimension tables for a star schema and build an ETL pipeline that transfers data from files in two local directories into these tables in Postgres.

## Data description

The dataset consists of two parts Song Dataset and Log Dataset.

Song Dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/), Each file is in JSON format and contains metadata about a song and the artist of that song.

Log Dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Database schema design

In order to optimize queries for analysis we need to create a star schema optimized for queries on song play analysis. 
This includes the following tables:

**Fact Table**

songplays: records in log data associated with song plays.


**Dimension Tables**

users: users in the app

songs: songs in the music database

artists: artists in the music database

time: timestamps of records in songplays broken down into specific units

## Files in the repository

1. **data**: contains all JSON files.
2. **sql_queries.py**: contains SQL queries for creating tables, inserting values to the tables and dropping tables
3. **create_tables.py**: Making the connection to the default database, executing drop and create queries imported from sql_queries.py
4. **test.ipynb**: Conduct Sanity Tests to check for errors in creation, insertion and dropping data.
5. **etl.ipynb**: Load a single JSON file from the data into the tables.
6. **etl.py**: Load the full JSON file from the data into the tables.
7. **ERD postgres.png**: star schema.

## How to run the Python scripts

First, run create_tables.py to create the tables which imports the queries in sql_queries.py, After that run test.ipynb to ensure every table is correctly created, Finally, run etl.py to load the data to the tables.


## Example queries


```python
%load_ext sql
%sql postgresql://student:student@127.0.0.1/sparkifydb
%sql SELECT count(*) FROM songs
```

>>> postgresql://student:***@127.0.0.1/sparkifydb

>>> 1 rows affected.

>>> Out[1]: count 71


# Second part 

### Data Modeling with Apache Cassandra

In the second part of the project, we will need to model our data by creating tables in Apache Cassandra to run queries. I was provided with part of the ETL pipeline that transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables. 

## Data description

For this project, you'll be working with one dataset: event_data. The directory of CSV files partitioned by date.

## Files in the repository

1. **event_data**: contains all csv files.
2. **event_datafile_new.csv**: the denormalized data used for Apache Casssandra tables should appear here.
3. **Data_Modeling_with_Apache_Cassandra.ipynb**: the notebook use for the second part of the project.

## How to run the Python script

Simply, run Data_Modeling_with_Apache_Cassandra.ipynb to create an Apache Cassandra database.
